{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing for Text Classification with NLTK and Scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the Dataset\n",
    "datset [link](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"SMSSpamCollection\",sep='\\t',names=[\"label\",\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# convert class labels to binary values, 0 = ham and 1 = spam only for 2 label in colume\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(dataset['label'])\n",
    "Y[1:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Input Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n"
     ]
    }
   ],
   "source": [
    "# Stemming-Stemming is process od reducing infected words to their root form.\n",
    "# Example-intelligence\n",
    "#         intelligent                intelligen\n",
    "#         intelligently\n",
    "\n",
    "# data cleaning and pre processing\n",
    "#re use for regular expression\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download(\"stopwords\") remove # for first time to downlode \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "corpus=[]\n",
    "print(dataset['message'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free entry in   a wkly comp to win FA Cup final tkts   st May       Text FA to       to receive entry question std txt rate T C s apply            over   s\n"
     ]
    }
   ],
   "source": [
    "#remove all word except alphabet using regular expression\n",
    "dummyText=re.sub('[^a-zA-Z]', ' ',dataset['message'][2])\n",
    "print(dummyText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free entry in   a wkly comp to win fa cup final tkts   st may       text fa to       to receive entry question std txt rate t c s apply            over   s\n"
     ]
    }
   ],
   "source": [
    "#change text to lower case\n",
    "dummyText=dummyText.lower()\n",
    "print(dummyText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['free', 'entry', 'in', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', 'over', 's']\n"
     ]
    }
   ],
   "source": [
    "#split the text in List\n",
    "dummyText=dummyText.split()\n",
    "print(dummyText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['free', 'entri', 'wkli', 'comp', 'win', 'fa', 'cup', 'final', 'tkt', 'st', 'may', 'text', 'fa', 'receiv', 'entri', 'question', 'std', 'txt', 'rate', 'c', 'appli']\n"
     ]
    }
   ],
   "source": [
    "#remove the Stopword from the List\n",
    "dummyText=[ps.stem(word) for word in dummyText if not word in stopwords.words('english')]\n",
    "print(dummyText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free entri wkli comp win fa cup final tkt st may text fa receiv entri question std txt rate c appli\n"
     ]
    }
   ],
   "source": [
    "#join the List Words in String \n",
    "dummyText=' '.join(dummyText)\n",
    "print(dummyText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ok lar joke wif u oni', 'free entri wkli comp win fa cup final tkt st may text fa receiv entri question std txt rate c appli', 'u dun say earli hor u c alreadi say', 'nah think goe usf live around though', 'freemsg hey darl week word back like fun still tb ok xxx std chg send rcv', 'even brother like speak treat like aid patent', 'per request mell mell oru minnaminungint nurungu vettam set callertun caller press copi friend callertun', 'winner valu network custom select receivea prize reward claim call claim code kl valid hour', 'mobil month u r entitl updat latest colour mobil camera free call mobil updat co free']\n"
     ]
    }
   ],
   "source": [
    "#do all above data preprocessing for data set using for loop\n",
    "for i in range(len(dataset['message'])):\n",
    "    email=re.sub('[^a-zA-Z]',' ',dataset['message'][i])\n",
    "    email=email.lower()\n",
    "    email=email.split()\n",
    "    email=[ps.stem(word) for word in email if not word in stopwords.words('english')]\n",
    "    email=' '.join(email)\n",
    "    corpus.append(email)\n",
    "print(corpus[1:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generating Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 6296)\n"
     ]
    }
   ],
   "source": [
    "# creating the beg of words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=25000)\n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Scikit-Learn Classifiers with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 98.56502242152466\n"
     ]
    }
   ],
   "source": [
    "# import to check model performance\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_LR=LogisticRegression()\n",
    "model_LR.fit(X_train,Y_train)\n",
    "Y_prect=model_LR.predict(X_test)    \n",
    "print(\"score\",accuracy_score(Y_test,Y_prect)*100)\n",
    "# print(pd.DataFrame(confusion_matrix(Y_test,Y_prect),index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "#     columns = [['predicted', 'predicted'], ['ham', 'spam']]))\n",
    "# print(classification_report(Y_test,Y_prect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.83856502242152\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbor Classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_KNN=KNeighborsClassifier()\n",
    "model_KNN.fit(X_train,Y_train)\n",
    "Y_prect=model_KNN.predict(X_test)    \n",
    "print(accuracy_score(Y_test,Y_prect)*100)\n",
    "# print(pd.DataFrame(confusion_matrix(Y_test,Y_prect),index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "#     columns = [['predicted', 'predicted'], ['ham', 'spam']]))\n",
    "# print(classification_report(Y_test,Y_prect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.7443946188341\n"
     ]
    }
   ],
   "source": [
    "#SVM model\n",
    "from sklearn.svm import SVC\n",
    "model_SVM =SVC(kernel = 'linear')\n",
    "model_SVM.fit(X_train,Y_train)\n",
    "Y_prect=model_SVM.predict(X_test)    \n",
    "print(accuracy_score(Y_test,Y_prect)*100)\n",
    "# print(pd.DataFrame(confusion_matrix(Y_test,Y_prect),index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "#     columns = [['predicted', 'predicted'], ['ham', 'spam']]))\n",
    "# print(classification_report(Y_test,Y_prect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.11659192825111\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeClassifier model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_DT=DecisionTreeClassifier()\n",
    "model_DT.fit(X_train,Y_train)\n",
    "Y_prect=model_DT.predict(X_test)    \n",
    "print(accuracy_score(Y_test,Y_prect)*100)\n",
    "# print(pd.DataFrame(confusion_matrix(Y_test,Y_prect),index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "#     columns = [['predicted', 'predicted'], ['ham', 'spam']]))\n",
    "# print(classification_report(Y_test,Y_prect))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.9372197309417\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_RF=RandomForestClassifier()\n",
    "model_RF.fit(X_train,Y_train)\n",
    "Y_prect=model_RF.predict(X_test)    \n",
    "print(accuracy_score(Y_test,Y_prect)*100)\n",
    "# print(pd.DataFrame(confusion_matrix(Y_test,Y_prect),index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "#     columns = [['predicted', 'predicted'], ['ham', 'spam']]))\n",
    "# print(classification_report(Y_test,Y_prect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.847533632287\n"
     ]
    }
   ],
   "source": [
    "# naive bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model_NB=RandomForestClassifier()\n",
    "model_NB.fit(X_train,Y_train)\n",
    "Y_prect=model_NB.predict(X_test)    \n",
    "print(accuracy_score(Y_test,Y_prect)*100)\n",
    "# print(pd.DataFrame(confusion_matrix(Y_test,Y_prect),index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "#     columns = [['predicted', 'predicted'], ['ham', 'spam']]))\n",
    "# print(classification_report(Y_test,Y_prect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
